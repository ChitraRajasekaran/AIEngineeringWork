{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b582f69-714b-4752-812f-d1383ab925ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c67fccf-c085-4c0a-a658-0712a8f4276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654460cc-5d61-4529-8e64-0039f8bccb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain langchain-core langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "759f0b7c-8c1a-41b9-898d-17d81d511985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f605e9-4c24-45c1-9779-992cc99834d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030332c6-b4ab-49c5-83d3-34c98a936fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90294c9c-21b3-41d7-9739-13ba1003e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ffa123-bea3-4e46-97bf-7a05ed4ef2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3: Initialize a Simple Chain using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d5f6d9-c6a7-4f57-baff-ca7042b3b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d97450-768b-45d7-8a88-fae1c39eaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc3fb5f-a7a8-4207-90b8-4056212eaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our first Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf44a6b-4665-41d0-ae44-3652c5378745",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | openai_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6450e8-3637-4460-aacb-5311ddff01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Greetings, traveler of the pixelated paths! As the digital dawn rises, may your journey be as smooth as brie and as adventurous as a Roquefort chase! What brings you to the realm of binary and sorcery today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 38, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-da687a9d-fc71-4413-a0ab-585fb999f7a3-0' usage_metadata={'input_tokens': 38, 'output_tokens': 49, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"content\": \"Hello world!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb1e598-fadf-448f-9376-a6aa5bca8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ah, young seeker of knowledge, gather 'round and hold tight to your magical hat! \\n\\nImagine, if you will, a grand parade of cheese! Yes, a delightful procession where each cheese is a different flavor and brings something unique to the table, much like the many threads of a magical story. Now, picture yourself as a curious little mouse named Cheddar. You want to taste every cheese but don't know how to line them up just right for the perfect cheesy experience.\\n\\nLangchain is like your wise mouse friend, Monterey Jack, who helps you connect these cheeses in the best order. Each cheese represents a piece of information or a magical trick. Monterey Jack helps you figure out how to use one piece before another, making sure your journey from Muenster to Mozzarella is as delightful as discovering that the moon is made of Gouda after all! \\n\\nIn the world of wizards and spells, a Langchain organizes how different magical languages and abilities come together to create something truly enchanting. So, while you may not be able to summon the moon with a trinitrotoluene of Truffle Tremor, Langchain will help your magical party be as smooth as a perfectly aged Brie.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 243, 'prompt_tokens': 47, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None}, id='run-86995786-3225-4664-a27b-1267b768516e-0', usage_metadata={'input_tokens': 47, 'output_tokens': 243, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"content\" : \"explain me Langchain like i am 5 year old\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "308bd62f-5f73-47c0-bd6b-467d0dc2ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits such as full sync, async, batch, and streaming support, enabling prototyping in Jupyter notebooks and exposing chains as async streaming interfaces. LCEL allows for easy attachment of fallbacks to handle errors gracefully due to the non-determinism of LLMs. It supports parallelism, allowing components that can run in parallel to do so automatically. Additionally, LCEL offers seamless integration with LangSmith Tracing, logging all steps for maximal observability and debuggability as chains grow more complex.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 274, 'total_tokens': 398, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None} id='run-5a7c6ee1-2afe-4690-b9f2-cfae51711537-0' usage_metadata={'input_tokens': 274, 'output_tokens': 124, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT = \"\"\"\n",
    "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "\n",
    "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "\n",
    "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f70ad5b-adfb-4fe7-a2ce-e428df7ae831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# Task #4: Implement Naive RAG using LCEL\n",
    "\n",
    "# Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e960c2-39ec-4f9d-a5c0-56e9e8687524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting the R in RAG: Retrieval 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ace97e6-1035-4ada-8e85-facee25c7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
    "\n",
    "# Let's tackle an immediate problem first: The Context Window.\n",
    "\n",
    "# All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
    "\n",
    "# Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!\n",
    "\n",
    "#     NOTE: It is recommended you do not run the following cells, they are purely for demonstrative purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "548d4a61-b6e7-48dd-b161-46e1a4890849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #context = \"\"\"\n",
    "# EVERY HITCHHIKER'S GUIDE BOOK\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2baa324-ef6d-4e3c-81dc-78031cbf48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can leverage our tokenizer to count the number of tokens for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b95eaebf-fec6-476b-971f-6a86fba620df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8963489-ab7f-4f49-8ba7-3156cac33647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(enc.encode(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4985c6a6-e1e1-45b1-a109-740975846721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 636144\n",
    "\n",
    "# The full set comes in at a whopping 636,144 tokens.\n",
    "\n",
    "# So, we have too much context. What can we do?\n",
    "\n",
    "# Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even gpt-4-128k wouldn't be able to fit that whole text in the context window at once.\n",
    "\n",
    "# So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
    "\n",
    "# We have another problem now.\n",
    "\n",
    "# If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
    "\n",
    "#     NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
    "\n",
    "# In order to conceptualize the following processes - let's create a toy context set!\n",
    "# TextSplitting aka Chunking\n",
    "\n",
    "# We'll use the RecursiveCharacterTextSplitter to create our toy example.\n",
    "\n",
    "# It will split based on the following rules:\n",
    "\n",
    "#     Each chunk has a maximum size of 100 tokens\n",
    "#     It will try and split first on the \\n\\n character, then on the \\n, then on the <SPACE> character, and finally it will split on individual tokens.\n",
    "\n",
    "# Let's implement it and see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0e1cdb2-c78b-4c29-82e5-e654aff3d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def tiktoken_len(text):\n",
    "#     tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "#         text,\n",
    "#     )\n",
    "#     return len(tokens)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 100,\n",
    "#     chunk_overlap = 0,\n",
    "#     length_function = tiktoken_len,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edce8d46-0f4e-4701-a25e-77b2f80f2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = text_splitter.split_text(CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d771f370-a75a-4a8f-99ba-90086d3e999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(chunks) o/p: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd07c63-ed35-4991-a435-c558494709c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in chunks:\n",
    "#   print(chunk)\n",
    "#   print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d0ed700-80c8-4602-b5c6-4ef159ff3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "# Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "# ----\n",
    "# Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "# Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "# ----\n",
    "# Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "# ----\n",
    "\n",
    "# As is shown in our result, we've split each section into 100 token chunks - cleanly separated by \\n\\n characters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5487266-80b3-48dd-8cb0-d879cf6ba11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7564e7f6-de59-49ec-8ff2-d07a16073b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embeddings and Dense Vector Search\n",
    "\n",
    "# Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
    "\n",
    "# This sounds like a perfect job for embeddings!\n",
    "\n",
    "# We'll be using OpenAI's text-embedding-3 model as our embedding model today!\n",
    "\n",
    "# Let's load it up through LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60a7e847-7304-4d13-87b0-a693d25923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a9fec65-6aab-4589-9a3d-595d6d66ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The embedding dimension for text-embedding-3-small is 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f29f64-ba81-40a2-93bb-8f8963bf63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finding the Embeddings for Our Chunks\n",
    "\n",
    "# First, let's find all our embeddings for each chunk and store them in a convenient format for later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25aeff88-2484-4e9a-8be7-396a377e16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict = {}\n",
    "\n",
    "# for chunk in chunks:\n",
    "#   embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c7a165-6efe-4838-ab3e-4642171064c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in embeddings_dict.items():\n",
    "#   print(f\"Chunk - {k}\")\n",
    "#   print(\"---\")\n",
    "#   print(f\"Embedding - Vector of Size: {len(v)}\")\n",
    "#   print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b655e4-30d9-4216-bf2e-4aa977250988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "# Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "# ---\n",
    "# Embedding - Vector of Size: 1536\n",
    "\n",
    "\n",
    "\n",
    "# Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "# Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "# ---\n",
    "# Embedding - Vector of Size: 1536\n",
    "\n",
    "\n",
    "\n",
    "# Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "# ---\n",
    "# Embedding - Vector of Size: 1536\n",
    "\n",
    "\n",
    "\n",
    "# Okay, great. Let's create a query - and then embed it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05784e96-acea-43a8-87ed-cd0fd7657b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Can LCEL help take code from the notebook to production?\"\n",
    "\n",
    "# query_vector = embedding_model.embed_query(query)\n",
    "# print(f\"Vector of Size: {len(query_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2c5ab7a-51e5-47d0-9357-4d37cb850348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Vector of Size: 1536\n",
    "\n",
    "# Now, let's compare it against each existing chunk's embedding by using cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80000b82-f33f-4f22-8d87-8e713b75f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# def cosine_similarity(vec_1, vec_2):\n",
    "#   return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "599e6842-75bf-4db0-aa38-ad8b58a5c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_similarity = -float('inf')\n",
    "# closest_chunk = \"\"\n",
    "\n",
    "# for chunk, chunk_vector in embeddings_dict.items():\n",
    "#   cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "#   if cosine_similarity_score > max_similarity:\n",
    "#     closest_chunk = chunk\n",
    "#     max_similarity = cosine_similarity_score\n",
    "\n",
    "# print(closest_chunk)\n",
    "# print(max_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a3e3bfb-b199-4a6d-bc78-182b7785c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "# Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "# 0.537298487051912\n",
    "\n",
    "# And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!\n",
    "# Creating a Retriever\n",
    "\n",
    "# Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
    "\n",
    "# First, we'll wrap the above in a helper function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55b24338-2fa7-4c99-8897-c9df54d8beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_context(query, embeddings_dict, embedding_model):\n",
    "#   query_vector = embedding_model.embed_query(query)\n",
    "#   max_similarity = -float('inf')\n",
    "#   closest_chunk = \"\"\n",
    "\n",
    "#   for chunk, chunk_vector in embeddings_dict.items():\n",
    "#     cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "#     if cosine_similarity_score > max_similarity:\n",
    "#       closest_chunk = chunk\n",
    "#       max_similarity = cosine_similarity_score\n",
    "\n",
    "#   return closest_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25bf8276-1927-4464-aed9-09a37d422bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's add it to our pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b883bd5b-126c-4b72-bcd9-97c63be8f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
    "#   context = retrieve_context(query, embeddings_dict, embedding_model)\n",
    "\n",
    "#   response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
    "\n",
    "#   return_package = {\n",
    "#       \"query\" : query,\n",
    "#       \"response\" : response,\n",
    "#       \"retriever_context\" : context\n",
    "#   }\n",
    "\n",
    "#   return return_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9306bb10-1027-44f7-aff7-3697a1f52434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db287c1f-e416-486f-8994-c37e91790e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #o/p {'query': 'Can LCEL help take code from the notebook to production?',\n",
    "#  'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. Since any chain constructed using LCEL will automatically have full sync, async, batch, and streaming support, this makes it easy to prototype a chain in a Jupyter notebook using the sync interface and then expose it as an async streaming interface for production.', response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 152, 'total_tokens': 216}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-feaeed29-4db3-4e7a-b9ef-2f512437537e-0', usage_metadata={'input_tokens': 152, 'output_tokens': 64, 'total_tokens': 216}),\n",
    "#  'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4db8dfcb-d4a3-4699-8149-7002315ed581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does LCEL do that makes it more reliable at scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b21498d-0e98-4e8a-82c5-8f6f74a812aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "#     Streaming so that users begin to see responses within a consistent timeframe.\n",
    "#     Protection against LLM failures. So, using parallism to make multiple calls in the event that one doesn't return a correctly formatted respsone.\n",
    "#     Information architecture so that \"bad docs\" don't cause retrieval issues which will distact from answer quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd5313-c57f-41ca-a22d-00d2ff389785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
