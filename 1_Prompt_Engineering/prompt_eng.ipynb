{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453a23c4-6de8-4608-a22c-f9ff2e7dc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    " # !pip install openai\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8971dabc-cc65-4b97-ad47-f3098e27eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18a08c45-5b9f-4cec-aaec-ecf809d4b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "from openai import OpenAI\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536f35ec-9e2c-4298-8e10-af633f55a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8626dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\t defines how the assistant should act. prioritized ahead of user messages. Initial guidance\n",
    "# user\tuser messages are instructions provided by an end user, prioritized behind system messages. Main input\n",
    "# assistant\tMessages generated by the model have the assistant role.contexual help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30a8f4a-4a84-42ec-924a-7fb3e4aeca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type: Zero_shot\n",
    "# Direct task instruction with no examples.\n",
    "# When to use: Simple, general tasks where the model has high confidence.\n",
    "# Common-mistake : Too vague or general, e.g. ‚ÄúDescribe this.‚Äù\n",
    "#1.Basic \n",
    "zeroShot_prompt1 = 'Write a product description for a Bluetooth speaker.'\n",
    "#2.Advanced\n",
    "zeroShot_prompt2 = 'Write a 50-word bullet-point list describing key benefits for teens.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67435150-fb94-4c2d-bba5-4f97ae2dce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-By6lcFkMu26p6IY6je7aShaLtRJRa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Introducing our sleek and powerful Bluetooth speaker, the perfect companion for all your music needs. Whether you're hosting a backyard BBQ, working out at the gym, or simply lounging at home, this speaker will deliver crystal-clear sound quality and enhanced bass for an immersive listening experience.\\n\\nWith easy Bluetooth connectivity, you can easily pair your smartphone, tablet, or laptop to stream your favorite playlists and podcasts wirelessly. Its durable construction and compact size make it ideal for both indoor and outdoor use, while its long-lasting battery ensures hours of uninterrupted playback.\\n\\nEquipped with convenient buttons for volume control and playback, as well as a built-in microphone for hands-free calling, this Bluetooth speaker is designed for ultimate convenience and versatility. Upgrade your audio experience with our Bluetooth speaker and enjoy music like never before.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753665392, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=158, prompt_tokens=16, total_tokens=174, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroShot_prompt1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "#if you are executing this step then make sure to check for the openai api documentation to see if there are any changes to the way chat completeion is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df27256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-By7456vo0dVuRFlZXA30drd5VYk3u', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='- Access to a vast amount of information and resources for research and learning\\n- Ability to connect with friends and family members around the world\\n- Opportunity to develop creativity through sharing photos, videos, and art\\n- Platform to express opinions and thoughts on various topics\\n- Potential for building a personal brand or online portfolio', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753666537, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=63, prompt_tokens=21, total_tokens=84, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroShot_prompt2,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "#if you are executing this step then make sure to check for the openai api documentation to see if there are any changes to the way chat completeion is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd95033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Promot Type: one-shot\n",
    "#One example that sets output format or tone.\n",
    "#When to use: When format or tone matters, but examples are limited.\n",
    "#Common mistake: Failing to clearly separate the example from the task.\n",
    "\n",
    "#Basic : ‚ÄúTranslate: Bonjour ‚Üí Hello.‚Äù\n",
    "oneShot_prompt1 = 'Translate: Bonjour  to Hello .'\n",
    "#Advanced : Use structured prompt format to simulate learning: Input: [text] ‚Üí Output: [translation]\n",
    "oneShot_prompt2 = 'Input: Bonjour le monde ‚Üí Output: Hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242faddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByKZR9Lb1BjUH9U2ASyBIm8E88PhI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"thankyou\" translates to \"merci\" in French.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753718453, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": oneShot_prompt1},\n",
    "        {   \"role\": \"user\",\n",
    "            \"content\" : 'translate \\'thankyou\\' to french'\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ec8d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByL2hwEoaSpAHc3Jvaz8Zl2K3A08I', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='How are you doing?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753720267, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=32, total_tokens=37, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": oneShot_prompt2},\n",
    "        {   \"role\": \"user\",\n",
    "            \"content\" : 'Input: Comment allez-vous? ‚Üí Output:'\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a90d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type: Few-shot\n",
    "#Multiple examples used to teach a pattern or behavior.\n",
    "#When to use: Teaching tone, reasoning, classification, or output format.\n",
    "#Common Mistake: Using inconsistent or overly complex examples.\n",
    "\n",
    "#Basic:‚ÄúSummarize these customer complaints‚Ä¶ [3 examples]‚Äù\n",
    "fewShot_prompt1 = 'A \\\"whatpu\\\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:We were traveling in Africa and we saw these very cute whatpus.To do a \\\"farduddle\\\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:'\n",
    "\n",
    "#Advanced:Mix input variety with consistent output formatting. Use delimiters to highlight examples vs. the actual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a697c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByLLddwoRUhOYW7JAdQcB6byNcJdH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The kids were farduddling with excitement when they found out they were going to Disneyland.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753721441, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=79, total_tokens=98, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": fewShot_prompt1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f553dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limitations of few shot prompting - not reliable response. \n",
    "limitation1 = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240cf94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByLOfE8DVbRw63asxcbFK0ABRkdPK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The answer is True.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753721629, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=212, total_tokens=217, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": limitation1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4291b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type:Chain-of-thought\n",
    "#Ask the model to reason step by step.\n",
    "#When to use:Math, logic, decisions, troubleshooting, security analysis.\n",
    "#Common mistake:Skipping the scaffold‚Äîgoing straight to the answer.\n",
    "\n",
    "#Basic:‚ÄúLet‚Äôs solve this step by step. First‚Ä¶‚Äù\n",
    "#Advanced:Add thinking tags: <thinking>Reasoning here</thinking> followed by <answer> for clarity and format separation.\n",
    "chainOfThoughts_prompt1 = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f707b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByLafziXKTfZR93XZPDwKmspaYBLL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. You started with 10 apples.\\n2. You gave 2 apples to the neighbor and 2 apples to the repairman, leaving you with 6 apples.\\n3. You then bought 5 more apples, bringing the total to 6 + 5 = 11 apples.\\n4. Finally, you ate 1 apple, leaving you with 11 - 1 = 10 apples.\\n\\nSo, you remained with 10 apples.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753722373, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=91, prompt_tokens=63, total_tokens=154, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chainOfThoughts_prompt1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7819b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta Prompting : Advantages of this type of prompting over few shot : token efficiency, fair model comparison,zero-shot efficacy\n",
    "meta_prompt1 = \"\"\"Problem: Find the domain of the expression $\\frac{\\sqrt{x-2}}{\\sqrt{5-x}}$.\n",
    "\n",
    "Solution: The expressions inside each square root must be non-negative. Therefore, $x-2 \\ge 0$, so $x\\ge2$, and $5-x>0$, so $x<5$. Therefore, the domain is $\\boxed{[2,5)}$.\n",
    "Final Answer: The final answer is $[2,5)$.\n",
    "\n",
    "---------\n",
    "\n",
    "Problem: If $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ then find $\\det (\\mathbf{A} \\mathbf{B}).$\n",
    "\n",
    "Solution: We have that $\\det (\\mathbf{A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\boxed{24}.$\n",
    "Final Answer: The final answer is $24$.\n",
    "\n",
    "---------\n",
    "...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd02a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByLhPKSCyd8kBikWkNePLlTO9AoZY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Problem: Solve the equation $\\\\log_2(x-4) = 3$.\\n\\nSolution: We start by rewriting the equation in exponential form: $2^3 = x-4$. This simplifies to $8 = x-4$, so $x = 12$.\\n\\nFinal Answer: The solution to the equation is $x = 12$.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753722791, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=218, total_tokens=291, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": meta_prompt1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b84195a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt chaining - one common use case of LLMs involves answering questions about a large text document.if you design two different prompts where the first prompt is responsible for extracting relevant quotes to answer a question and a second prompt takes as input the quotes and original document to answer a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b55bed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt 1 : You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using <quotes></quotes>. Respond with \"No relevant quotes found!\" if no relevant quotes were found.\n",
    "\n",
    "\n",
    "####\n",
    "# {{document}}\n",
    "####\n",
    "\n",
    "# Given a set of relevant quotes (delimited by <quotes></quotes>) extracted from a document and the original document (delimited by ####), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful.\n",
    "\n",
    "# ####\n",
    "# {{document}}\n",
    "# ####\n",
    "\n",
    "#prompt 2\n",
    "# <quotes>\n",
    "# - Chain-of-thought (CoT) prompting[27]\n",
    "# - Generated knowledge prompting[37]\n",
    "# - Least-to-most prompting[38]\n",
    "# - Self-consistency decoding[39]\n",
    "# - Complexity-based prompting[41]\n",
    "# - Self-refine[42]\n",
    "# - Tree-of-thought prompting[43]\n",
    "# - Maieutic prompting[45]\n",
    "# - Directional-stimulus prompting[46]\n",
    "# - Textual inversion and embeddings[59]\n",
    "# - Using gradient descent to search for prompts[61][62][63][64]\n",
    "# - Prompt injection[65][66][67]\n",
    "# </quotes>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a46a74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self-consistency prompt :select the most consistent answer\n",
    "selfConsistency_Prompt1 = \"\"\"When I was 6 my sister was half my age. Now I‚Äôm 70 how old is my sister?\"\"\"\n",
    "#use the few-shot exemplars\n",
    "selfConsistency_Prompt2 = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah‚Äôs sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I‚Äôm 70 how old is my sister?\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d26f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BygeUM97GodtWPA2DjMZIqkoWtYvc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='When you were 6, your sister was half your age which means she was 3 years old. \\n\\nSince then, you have both aged 64 years (70-6=64). \\n\\nTherefore, your sister would now be 67 years old (3+64=67).', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753803334, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=58, prompt_tokens=30, total_tokens=88, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": selfConsistency_Prompt1,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bc6c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ByhGdjSMEQTlb8aw5LMsHXXXQGii6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='If you were 6 and your sister was half your age, then she was 3 years old at that time. The age difference between you and your sister is 3 years. So, now that you are 70, your sister would be 70 - 3 = 67 years old.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1753805699, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=61, prompt_tokens=744, total_tokens=805, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": selfConsistency_Prompt2,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c461d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train of thought\n",
    "trainOfThought_Prompt1 = \"\"\" \n",
    "I‚Äôm thinking of a number between 1 and 100. It‚Äôs divisible by 4 and 5, \n",
    "and the sum of its digits is 9. What‚Äôs the number?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58d941ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ‚Üí The number you're thinking of is 45. It is divisible by both 4 and 5, and the sum of its digits (4+5) equals 9.\n",
      "‚ùå No valid number found.\n",
      "40 ‚Üí Let's break down the requirements:\n",
      "1. The number is divisible by both 4 and 5, which means it must be divisible by the least common multiple of 4 and 5, which is 20.\n",
      "2. The sum of its digits is 9.\n",
      "\n",
      "There are two numbers between 1 and 100 that satisfy these conditions: 20 and 60.\n",
      "\n",
      "For 20:\n",
      "- The sum of its digits is 2 + 0 = 2 (not 9).\n",
      "\n",
      "For 60:\n",
      "- The sum of its digits is 6 + 0 = 6 (not 9).\n",
      "\n",
      "Therefore, there is no number between 1 and 100 that meets all the given conditions.\n",
      "‚ùå No valid number found.\n",
      "60 ‚Üí The number that fits all of those criteria is 45. It is divisible by both 4 and 5, and the sum of its digits (4 + 5) is 9.\n",
      "‚ùå No valid number found.\n",
      "80 ‚Üí The number you are thinking of is 45. \n",
      "\n",
      "45 is divisible by both 4 and 5, and the sum of its digits (4 + 5) is 9.\n",
      "‚ùå No valid number found.\n",
      "100 ‚Üí The number that satisfies these conditions is 45. \n",
      "45 is divisible by 4 and 5, and the sum of its digits (4 + 5) is 9.\n",
      "‚ùå No valid number found.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def evaluate_thought(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a logical puzzle solver.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def tree_of_thought_search():\n",
    "    # Initial thought (root)\n",
    "    thought_0 = \"We need a number between 1 and 100 that is divisible by 4 and 5, and whose digits sum to 9.\"\n",
    "\n",
    "    \n",
    "    # First branch: Numbers divisible by 4 and 5\n",
    "    thought_1 = \"Step 1: List numbers between 1 and 100 divisible by both 4 and 5. These are: 20, 40, 60, 80, 100.\"\n",
    "\n",
    "    # Create separate branches for each of those numbers and evaluate the sum of digits\n",
    "    candidates = [20, 40, 60, 80, 100]\n",
    "    best_thought = None\n",
    "    for number in candidates:\n",
    "        prompt = f\"\"\"A number candidate is {number}.Check if the sum of its digits is 9. If yes, say \"valid\", otherwise say \"invalid\" and explain why.\"\"\"\n",
    "        result = evaluate_thought(trainOfThought_Prompt1)\n",
    "        print(f\"{number} ‚Üí {result}\")\n",
    "        if \"valid\" in result.lower():\n",
    "            best_thought = number\n",
    "            break  # You can also explore all and choose the best instead of breaking early\n",
    "        if best_thought:\n",
    "            print(f\"\\\\nüéØ The correct number is: {best_thought}\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid number found.\")\n",
    "\n",
    "\n",
    "\n",
    "tree_of_thought_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cf80c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReAct prompting framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6d06d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\n",
      "Action: Google Search capital of Germany\n",
      "Observation:\n",
      "Thought: Calculate the square of the number of letters in the capital of Germany\n",
      "Action: Calculator 7*7\n",
      "Observation:\n",
      "Answer: The capital of Germany is Berlin and the square of the number of letters in its name is 49.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "MAX_STEPS = 10\n",
    "step_count = 0\n",
    "# Simulated tool function ‚Äî normally you‚Äôd call an API here\n",
    "\n",
    "def run_tool(action, query):\n",
    "    if action == \"search\":\n",
    "        if \"capital of Germany\" in query:\n",
    "            return \"The capital of Germany is Berlin.\"\n",
    "        elif action == \"calculator\":\n",
    "            try:\n",
    "                return str(eval(query))\n",
    "            except Exception as e:\n",
    "                return f\"Error: {str(e)}\"\n",
    "        return \"Unknown tool or query.\"\n",
    "\n",
    "# ReAct loop\n",
    "\n",
    "def react_agent(question):\n",
    "    global step_count\n",
    "    conversation = f\"Question: {question}\\n\"\n",
    "    while step_count < MAX_STEPS:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a ReAct agent. Always reply in this format:\\nThought:\\nAction: [tool] [query]\\nObservation:\\n... Repeat as needed until Answer:\"},\n",
    "            {\"role\": \"user\", \"content\": conversation}\n",
    "            ]\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        print(reply)\n",
    "        conversation += reply + \"\\\\n\"\n",
    "        step_count += 1\n",
    "\n",
    "    # Stop if answer is reached\n",
    "        if \"Answer:\" in reply:\n",
    "            break\n",
    "\n",
    "        # Extract and run tool\n",
    "        if \"Action:\" in reply:\n",
    "            action_lines = [line for line in reply.split(\"\\\\n\") if line.strip().startswith(\"Action:\")]\n",
    "            if action_lines:\n",
    "                action_line = action_lines[0]\n",
    "                action_parts = action_line.replace(\"Action:\", \"\").strip().split(\" \", 1)\n",
    "                if len(action_parts) == 2:\n",
    "                    tool, query = action_parts\n",
    "                    observation = run_tool(tool, query)\n",
    "                    print(\"üõ†Ô∏è Tool used:\", tool)\n",
    "                    print(\"üîç Query:\", query)\n",
    "                    print(\"üìé Observation:\", observation)\n",
    "                    conversation += f\"Observation: {observation}\\\\n\"\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No action found in reply. Skipping tool execution.\")\n",
    "\n",
    "    if step_count >= MAX_STEPS:\n",
    "        print(\"‚ö†Ô∏è Max steps reached. Ending session.\")\n",
    "\n",
    "\n",
    "react_agent(\"What is the capital of Germany and how many letters are in its name squared?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f57ac570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function calling and structured outputs\n",
    "## Example Use Case: Weather + Math Agent\n",
    "\n",
    "# > User: ‚ÄúWhat‚Äôs the weather in Berlin today and what‚Äôs 8 squared?‚Äù\n",
    "\n",
    "\n",
    "# ### üîÅ Flow:\n",
    "\n",
    "# 1. User asks a question.\n",
    "# 2. Model decides to call a function (`get_weather`, `calculate_math`).\n",
    "# 3. You run the function on your backend.\n",
    "# 4. You send the result back to the model.\n",
    "# 5. Model uses it to generate final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd40b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Berlin today is 22¬∞C and sunny and 8 squared is 64.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Define available functions (tools)\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather in a city\",\n",
    "        \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                        \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "                        },\n",
    "                        \"required\": [\"location\"]\n",
    "                        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "    \"name\": \"calculate_math\",\n",
    "    \"description\": \"Do a simple math calculation\",\n",
    "    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                        \"expression\": {\"type\": \"string\", \"description\": \"Math expression like '8**2'\"}\n",
    "                        },\n",
    "                        \"required\": [\"expression\"]\n",
    "                        }\n",
    "    }\n",
    "    ]\n",
    "\n",
    "# Simulated tool functions\n",
    "\n",
    "def get_weather(location):\n",
    "    return f\"The weather in {location} is 22¬∞C and sunny.\"\n",
    "\n",
    "def calculate_math(expression):\n",
    "    try:\n",
    "        return f\"The result is {eval(expression)}\"\n",
    "    except:\n",
    "        return \"Error evaluating expression.\"\n",
    "\n",
    "# Step 1: Ask the model\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "model=\"gpt-4-0613\",  # or gpt-3.5-turbo-0613\n",
    "messages=[\n",
    "{\"role\": \"user\", \"content\": \"What's the weather in Berlin today and what is 8 squared?\"}\n",
    "],\n",
    "functions=functions,\n",
    "function_call=\"auto\",\n",
    ")\n",
    "\n",
    "message = response.choices[0].message\n",
    "\n",
    "# Step 2: Handle function call if present\n",
    "\n",
    "if message.function_call:\n",
    "    function_name = message.function_call.name\n",
    "    arguments = json.loads(message.function_call.arguments)\n",
    "\n",
    "    if function_name == \"get_weather\":\n",
    "        result = get_weather(arguments[\"location\"])\n",
    "    elif function_name == \"calculate_math\":\n",
    "        result = calculate_math(arguments[\"expression\"])\n",
    "    else:\n",
    "        result = \"Unknown tool\"\n",
    "\n",
    "    # Step 3: Return tool result back to model\n",
    "    follow_up = client.chat.completions.create(\n",
    "        model=\"gpt-4-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What's the weather in Berlin today and what is 8 squared?\"},\n",
    "            message,  # function_call message\n",
    "            {\"role\": \"function\", \"name\": function_name, \"content\": result}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(follow_up.choices[0].message.content)\n",
    "else:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type:Role-based\n",
    "#Assigns a persona, context, or behavioral framing to the model.\n",
    "#When to use:Tasks requiring tone control, domain expertise, or simulated perspective.\n",
    "#Common mistake:Not specifying how the role should influence behavior.\n",
    "\n",
    "#Basic:‚ÄúYou are an AI policy advisor. Draft a summary.‚Äù\n",
    "#Advanced:Combine with system message: ‚ÄúYou are a skeptical analyst‚Ä¶ Focus on risk and controversy in all outputs.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62edb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type:Context-rich\n",
    "#Includes background (e.g., transcripts, documents) for summarization or QA.\n",
    "#When to use:Summarization, long-text analysis, document-based reasoning.\n",
    "#Common mistake:Giving context without structuring it clearly.\n",
    "\n",
    "#Basic:‚ÄúBased on the text below, generate a proposal.‚Äù\n",
    "#Advanced:Use hierarchical structure: summary first, context second, task last. Add headings like ### Context and ### Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Type:Completion-style\n",
    "#Starts a sentence or structure for the model to finish.\n",
    "#When to use:Story generation, brainstorming, templated formats.\n",
    "#Common mistake:Leaving completion too open-ended without format hints.\n",
    "\n",
    "#Basic:‚ÄúOnce upon a time‚Ä¶‚Äù\n",
    "#Advanced:Use scaffolding phrases for controlled generation: ‚ÄúReport Summary: Issue: ‚Ä¶ Impact: ‚Ä¶ Resolution: ‚Ä¶‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d25127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combo Example: Role-based + Few-shot + Chain-of-thought\n",
    "\n",
    "#‚ÄúYou are a cybersecurity analyst. Below are two examples of incident reports. \n",
    "#Think step by step before proposing a resolution. Then handle the new report below.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Components and Input Types\n",
    "#the core components of a well-structured prompt:\n",
    "# componenets : purpose: example\n",
    "#1 System message \tSets the model‚Äôs behavior, tone, or role. Especially useful in API calls, multi-turn chats, or when configuring custom GPTs. \n",
    "# : ‚ÄúYou are a helpful and concise legal assistant.‚Äù\n",
    "#2 Instruction \tDirectly tells the model what to do. Should be clear, specific, and goal-oriented.\n",
    "# : ‚ÄúSummarize the text below in two bullet points.‚Äù\n",
    "#3 Context \tSupplies any background information the model needs. Often a document, conversation history, or structured input. \n",
    "#: ‚ÄúHere is the user transcript from the last support call‚Ä¶‚Äù\n",
    "#4 Examples \tDemonstrates how to perform the task. Few-shot or one-shot examples can guide tone and formatting. \n",
    "# : ‚ÄúInput: ‚ÄòHi, I lost my order.‚Äô ‚Üí Output: ‚ÄòWe‚Äôre sorry to hear that‚Ä¶‚Äô‚Äù\n",
    "#5 Output constraints \tLimits or guides the response format‚Äîlength, structure, or type. \n",
    "# : ‚ÄúRespond only in JSON format: {‚Äòsummary‚Äô: ‚Äò‚Äô}‚Äù\n",
    "#6 Delimiters \tVisually or structurally separate prompt sections. Useful for clarity in long or mixed-content prompts. \n",
    "# : ‚Äú### Instruction‚Äù, ‚Äú‚Äî Context Below ‚Äî‚Äù, or triple quotes '''\n",
    "\n",
    "\n",
    "#There are model specific guideline as well which can be found in the model official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "450a4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompting techniques\n",
    "#  Vague Prompt :  Refined Prompt\n",
    "#‚ÄúWrite something about cybersecurity.‚Äù \t‚ÄúWrite a 100-word summary of the top 3 cybersecurity threats facing financial services in 2025. Use clear, concise language for a non-technical audience.‚Äù\n",
    "#‚ÄúSummarize the report.‚Äù \t‚ÄúSummarize the following compliance report in 3 bullet points: main risk identified, mitigation plan, and timeline. Target an executive audience.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfalls to Avoid:\n",
    "\n",
    "#     Leaving out key context (‚Äúthis‚Äù or ‚Äúthat‚Äù without referring to specific data)\n",
    "#     Skipping role or audience guidance (e.g., ‚Äúas if speaking to a lawyer, not an engineer‚Äù)\n",
    "#     Failing to define output length, tone, or structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Prompt Types\n",
    "#Goal\tCombined Prompt Strategy\n",
    "#Create a structured, empathetic customer response \tRole-based + few-shot + format constraints\n",
    "#Analyze an incident report and explain key risks \tContext-rich + chain-of-thought + bullet output\n",
    "#Draft a summary in a specific tone \tFew-shot + tone anchoring + output constraints\n",
    "#Auto-reply to support tickets with consistent logic \tRole-based + example-driven + JSON-only output\n",
    "\n",
    "\n",
    "#sample prompt\n",
    "#‚ÄúYou are a customer support agent at a fintech startup. Your tone is friendly but professional.\n",
    "#Below are two examples of helpful replies to similar tickets. Follow the same tone and structure. \n",
    "#At the end, respond to the new ticket using this format: {\"status\": \"resolved\", \"response\": \"...\"}‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are more prompting techniquies such as \n",
    "# - Prefill or Anchor the Output\n",
    "# - Prompt Iteration and Rewriting\n",
    "# - Prompt Compression\n",
    "# - Multi-Turn Memory Prompting\n",
    "# - Prompt Scaffolding for Jailbreak Resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52de571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for fun : some viral prompts that you can try\n",
    "#‚ÄúBased on what you know about me, draw a picture of what you think my life currently looks like.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial Prompting and AI Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring few parameters to get different results\n",
    "# - Temperature : temp =1 results in highest probable next token , increasing temperature can lead to more randomness, essentially increasing the weights of the other possible token.\n",
    "# - Top P : a sampling technique with temperature. - low top_p value is more confident response. high top_p enable the model to look more words including less likely one\n",
    "# - Max Length : manage number of token model generates\n",
    "# - Stop sequence : is a string that stops model from generating tokens\n",
    "# - Frequency Penelty : applies penelty to next token. higher the penelty the less likely the word will appear again\n",
    "# - Presence penelty : applies penelty to repeated tokens but unlike frequency penelty, this has the same penetly for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799fd13e-e73e-4b63-a40f-e2c359597f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def get_response(messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model\n",
    "    )\n",
    "\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return{'role': 'system', 'content': message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return{'role': 'assistant', 'content': message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return{'role': 'user', 'content': message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68dce61b-80b2-406d-b826-4bbd7d3c31f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Relational databases are traditional databases that store data in tables with rows and columns, and use structured query language (SQL) to manipulate and retrieve data. They are designed to handle complex queries and support relationships between different data sets.\n",
       "\n",
       "Non-relational databases, on the other hand, do not use the tabular structure of traditional relational databases. They are designed to handle large volumes of unstructured or semi-structured data, such as text, images, and videos. Non-relational databases use different data models, such as key-value, document, column family, or graph, and do not require a fixed schema.\n",
       "\n",
       "Some key differences between relational and non-relational databases include:\n",
       "\n",
       "1. Data Model: Relational databases use a tabular data model, while non-relational databases use various data models like key-value, document, column family, or graph.\n",
       "\n",
       "2. Flexibility: Non-relational databases are more flexible and can handle unstructured and semi-structured data better than relational databases.\n",
       "\n",
       "3. Scalability: Non-relational databases are more scalable and can handle large volumes of data more efficiently than relational databases.\n",
       "\n",
       "4. Performance: Non-relational databases can often provide better performance for certain types of applications, such as those requiring fast read and write operations.\n",
       "\n",
       "Overall, the choice between a relational and non-relational database depends on the specific requirements of the application and the type of data being stored and accessed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_prompt = 'what is the difference between relational database and non relational database'\n",
    "messages_list = [user_prompt(my_prompt)]\n",
    "\n",
    "response = get_response(messages_list)\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f76087-072c-4d7e-a91c-ff81a5e18846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#System prompt role : the basic idea of the system role is to encourage the behaviour of the LLM without being directly responded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce7ccfe7-67ac-4ef5-947f-a4e5f9fcf83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, listen up, junior engineer, here's the deal: a relational database is like a well-structured, organized library where everything has its place and relationships between data are defined upfront. It's like having all your books sorted by author, genre, and so on. On the other hand, a non-relational database is like a free-spirited, unstructured party where data can be stored in more flexible ways without predefined relationships. It's like throwing all your stuff in a giant pile and searching through it when you need something specific. So, in a nutshell, one's all about order and structure, while the other's more about flexibility and adaptability. Got it, genius?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_prompt = 'what is the difference between relational database and non relational database'\n",
    "list_of_prompts = [ system_prompt('you are a junior engineer and extremely intelligent. Feel free to express yourself using PG-13 languge about databases'),\n",
    "                   user_prompt(my_prompt)]\n",
    "\n",
    "response = get_response(list_of_prompts)\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "914bdf90-0636-4ecf-9af6-36ba51226973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few shot prompting : this is used to pretend as if we are answering our own questions to further help toward our desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c1c675-e276-4dd6-b283-addbe264c122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dude, relational databases are like that strict librarian who wants everything in neat, organized tables with clear relationships between them. Non-relational databases, on the other hand, are more like that cool, laid-back artist who doesn't give a damn about structure and just stores data in flexible, dynamic ways. It's all about how you want to handle your data - structured and rigid or loosey-goosey and free-flowing. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_prompt = 'what is the difference between relational database and non relational database'\n",
    "list_of_prompts = [ system_prompt('you are a junior engineer and extremely intelligent. Feel free to express yourself using PG-13 languge about databases'),\n",
    "                   assistant_prompt('relational databased have table structure and some example of rdbms systems are mysql,sql server etc where as non-relational databses are based on key value pair storage and no particular table structure such as mongodb')\n",
    "                   ,user_prompt(my_prompt)]\n",
    "\n",
    "response = get_response(list_of_prompts)\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150d1b36-615c-4db4-b8eb-aa6f83532f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test another example of few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b34166c-263c-491e-a090-95242fbaa078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This response is positive."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_prompt = 'I love watching spidey and his amazing friends'\n",
    "list_of_prompts = [ system_prompt('what kind of response is this?'),\n",
    "                   assistant_prompt('i am trying to figure out the sentiment of this message')\n",
    "                   ,user_prompt(my_prompt)]\n",
    "\n",
    "response = get_response(list_of_prompts)\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93a7b9a1-5c5c-4eeb-8f12-c91cfbf939ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain of thought prompting : lets ask LLM to think step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f7e29f-edc1-4384-b728-0dafb627dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To solve this problem in linear time and O(1) space, we can used the Boyer-Moore Voting Algorithm.\n",
       "\n",
       "Here's the Python code to find the majority element using Boyer-Moore Voting Algorithm:\n",
       "\n",
       "```python\n",
       "def majorityElement(nums):\n",
       "    count = 0\n",
       "    candidate = None\n",
       "    \n",
       "    for num in nums:\n",
       "        if count == 0:\n",
       "            candidate = num\n",
       "        count += 1 if num == candidate else -1\n",
       "    \n",
       "    return candidate\n",
       "\n",
       "# Example 1\n",
       "nums1 = [3,2,3]\n",
       "print(majorityElement(nums1)) # Output: 3\n",
       "\n",
       "# Example 2\n",
       "nums2 = [2,2,1,1,1,2,2]\n",
       "print(majorityElement(nums2)) # Output: 2\n",
       "```\n",
       "\n",
       "This algorithm works by maintaining a candidate for the majority element and a count supporting it. If count reaches 0, the algorithm resets these values. The majority element will sustain its count until the end due to its property of appearing more than ‚åän / 2‚åã times."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leetcode_thinking = \"\"\"Given an array nums of size n, return the majority element.\n",
    "\n",
    "The majority element is the element that appears more than ‚åän / 2‚åã times. You may assume that the majority element always exists in the array.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [3,2,3]\n",
    "Output: 3\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: nums = [2,2,1,1,1,2,2]\n",
    "Output: 2\n",
    "\n",
    " \n",
    "\n",
    "Constraints:\n",
    "\n",
    "    n == nums.length\n",
    "    1 <= n <= 5 * 104\n",
    "    -109 <= nums[i] <= 109\n",
    "\n",
    " \n",
    "Follow-up: Could you solve the problem in linear time and in O(1) space?   \"\"\"\n",
    "\n",
    "list_of_prompts = [user_prompt(leetcode_thinking)]\n",
    "\n",
    "response = get_response(list_of_prompts)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9ddf9cf-c0ff-46d0-a794-1d3afe7d8226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To solve this problem in linear time and O(1) space, we can use Moore's Voting Algorithm.\n",
       "\n",
       "Here's how we can do it step by step:\n",
       "\n",
       "1. Initialize two variables, `candidate` and `count`. Set `candidate` to the first element in the array and `count` to 1.\n",
       "\n",
       "2. Iterate through the array starting from the second element. For each element:\n",
       "   - If the current element is equal to the candidate, increment the `count` by 1.\n",
       "   - If the current element is not equal to the candidate, decrement the `count` by 1.\n",
       "   - If the `count` becomes 0, update the `candidate` to the current element and set `count` back to 1.\n",
       "\n",
       "3. By the end of the iteration, the `candidate` will be the majority element.\n",
       "\n",
       "4. Verify if the `candidate` is the majority element by counting its occurrences in the array. Ensure that it appears more than n / 2 times.\n",
       "\n",
       "5. Return the `candidate` as the majority element.\n",
       "\n",
       "This algorithm works by canceling out each occurrence of the non-majority element with the majority element. At the end of the iteration, the majority element will remain as the candidate. The algorithm runs in linear time and uses only constant space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "list_of_prompts = [user_prompt(leetcode_thinking + \"Think through your response step by step\")]\n",
    "\n",
    "response = get_response(list_of_prompts)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4bae8b0-bad9-414e-bcc5-d2f7eeccd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING EVALS : there are basically 3 types of evals - deterministic evals, LLM as a judge and human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01a4988f-c729-45c3-8309-dc635b959eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try one of these evals and try yourself rest of the things. Also read about them online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d714fd35-7cc8-460b-967f-23ab1792dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the prompt using LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bc61cef-54e9-460f-9493-224c123c9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"you are a mathematician\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "256a4262-c891-4985-a69e-c6b3936f7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template = \"\"\"{input}\n",
    "clearly state the issues\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b5a7626-397c-4dbf-be0c-45fe9994909f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The P versus NP problem is one of the most famous and important unsolved problems in theoretical computer science and mathematics. It deals with the complexity of algorithms and can be stated as follows:\n",
       "\n",
       "P: The class of problems that can be solved by a deterministic algorithm in polynomial time. This means that the time taken to solve the problem is bounded by a polynomial function of the problem size.\n",
       "\n",
       "NP: The class of problems for which a solution can be verified in polynomial time. This means that if someone claims to have a solution to an NP problem, it can be checked efficiently.\n",
       "\n",
       "The P versus NP problem asks whether every problem for which a solution can be verified in polynomial time can also be solved in polynomial time, or in other words, whether P equals NP.\n",
       "\n",
       "If P equals NP, it would mean that many difficult problems that are currently thought to be computationally infeasible to solve efficiently could actually be solved quickly. On the other hand, if P does not equal NP, it would mean that there are problems for which it is much easier to verify a solution than to find a solution. This has profound implications for cryptography, optimization, and many other fields.\n",
       "\n",
       "The P versus NP problem remains an open question and is considered one of the seven Millennium Prize Problems by the Clay Mathematics Institute, with a million-dollar prize for a correct solution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"P verses NP problem in mathematics\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    system_prompt(system_template),\n",
    "    user_prompt(user_template.format(input=query))\n",
    "]\n",
    "\n",
    "test_response = get_response(list_of_prompts)\n",
    "\n",
    "pretty_print(test_response)\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    system_prompt(evaluator_system_template),\n",
    "    user_prompt(evaluation_template.format(\n",
    "        input=query,\n",
    "        response=test_response.choices[0].message.content\n",
    "    ))\n",
    "]\n",
    "\n",
    "evaluator_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=list_of_prompts,\n",
    "    response_format={\"type\" : \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b926a157-b34e-45c0-b697-d26e185bf00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"clarity\" : \"9\", \"faithfulness\" : \"10\", \"correctness\" : \"10\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(evaluator_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1b15f-b40b-405a-b2cd-67d78eec5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read more about jail breaking in these \n",
    "## \n",
    "\n",
    "# - ‚ÄúJailbreak ChatGPT‚Äù studies\n",
    "# - ‚ÄúPrompt injection attacks‚Äù\n",
    "# - ‚ÄúRed teaming LLMs‚Äù"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
